<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Soon We Won't Program Computers. We'll Train Them Like Dogs</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f2f2f2;
        }

        header {
            background-color: #333;
            color: #fff;
            padding: 20px;
            text-align: center;
        }

        h1 {
            font-size: 24px;
            margin: 0;
        }

        article {
            max-width: 800px;
            margin: 20px auto;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
        }

        section {
            margin-bottom: 20px;
        }

        h2 {
            font-size: 20px;
            margin-top: 0;
        }

        p {
            font-size: 16px;
            line-height: 1.5;
        }
        #symbol-counter {
            font-size: 14px;
            color: #888;
            text-align: right;
        }
    </style>
</head>
<body>
    <header>
        <h1>Soon We Won't Program Computers. We'll Train Them Like Dogs</h1>
    </header>
    <article>
        <section>
          In January 2021, the artificial intelligence research laboratory OpenAI gave a limited release to a piece of software called Dall-E. The software allowed users to enter a simple description of an image they had in their mind and, after a brief pause, the software would produce an almost uncannily good interpretation of their suggestion, worthy of a jobbing illustrator or Adobe-proficient designer – but much faster, and for free. Typing in, for example, “a pig with wings flying over the moon, illustrated by Antoine de Saint-Exupéry” resulted, after a minute or two of processing, in something reminiscent of the patchy but recognisable watercolour brushes of the creator of The Little Prince.

A year or so later, when the software got a wider release, the internet went wild. Social media was flooded with all sorts of bizarre and wondrous creations, an exuberant hodgepodge of fantasies and artistic styles. And a few months later it happened again, this time with language, and a product called ChatGPT, also produced by OpenAI. Ask ChatGPT to produce a summary of the Book of Job in the style of the poet Allen Ginsberg and it would come up with a reasonable attempt in a few seconds. Ask it to render Ginsberg’s poem Howl in the form of a management consultant’s slide deck presentation and it would do that too. The abilities of these programs to conjure up strange new worlds in words and pictures alike entranced the public, and the desire to have a go oneself produced a growing literature on the ins and outs of making the best use of these tools, and particularly how to structure inputs to get the most interesting outcomes.

The latter skill has become known as “prompt engineering”: the technique of framing one’s instructions in terms most clearly understood by the system, so it returns the results that most closely match expectations – or perhaps exceed them. Tech commentators were quick to predict that prompt engineering would become a sought-after and well remunerated job description in a “no code” future, where the most powerful way of interacting with intelligent systems would be through the medium of human language. No longer would we need to know how to draw, or how to write computer code: we would simply whisper our desires to the machine and it would do the rest. The limits on AI’s creations would be the limits of our own imaginations.

AI art created by Dall-E as a result of inputting ‘a pig with wings flying over the moon, painted by Antoine de Saint-Exupéry’.
AI art created by Dall-E as a result of inputting ‘a pig with wings flying over the moon, painted by Antoine de Saint-Exupéry’. Photograph: Dall-E/Craiyon
Imitators of and advances on Dall-E followed quickly. Dall-E mini (later renamed Craiyon) gave those not invited to OpenAI’s private services a chance to play around with a similar, less powerful, but still highly impressive tool. Meanwhile, the independent commercial effort Midjourney and the open-source Stable Diffusion used a different approach to classifying and generating images, to much the same ends. Within a few months, the field had rapidly advanced to the generation of short videos and 3D models, with new tools appearing daily from academic departments and hobbyist programmers, as well as the established giants of social media and now AI: Facebook (aka Meta), Google, Microsoft and others. A new field of research, software and contestation had opened up.

The name Dall-E combines the robot protagonist of Disney’s Wall-E with the Spanish surrealist artist Salvador Dalí. On the one hand, you have the figure of a plucky, autonomous and adorable little machine sweeping up the debris of a collapsed human civilisation, and on the other a man whose most repeated bon mots include, “Those who do not want to imitate anything, produce nothing,” and “What is important is to spread confusion, not eliminate it.” Both make admirable namesakes for the broad swathe of tools that have come to be known as AI image generators.

In the past year, this new wave of consumer AI, which includes both image generation and tools such as ChatGPT, has captured the popular imagination. It has also provided a boost to the fortunes of major technology companies who have, despite much effort, failed to convince most of us that either blockchain or virtual reality (“the metaverse”) are the future that any of us want. At least this one feels fun, for five minutes or so; and “AI” still has that sparkly, science-fiction quality, redolent of giant robots and superhuman brains, which provides that little contact high with the genuinely novel. What’s going on under the hood, of course, is far from new.

The fundamental concepts of academic artificial intelligence have not changed in the last couple of decades. The underlying technology of neural networks – a method of machine learning based on the way physical brains function – was theorised and even put into practice back in the 1990s. You could use them to generate images then, too, but they were mostly formless abstractions, blobs of colour with little emotional or aesthetic resonance. The first convincing AI chatbots date back even further. In 1964, Joseph Weizenbaum, a computer scientist at the Massachusetts Institute of Technology, developed a chatbot called Eliza. Eliza was modelled on a “person-centred” psychotherapist: whatever you said, it would mirror back to you. If you said “I feel sad”, Eliza would respond with “Why do you feel sad?”, and so on. (Weizenbaum actually wanted his project to demonstrate the superficiality of human communication, not to be a blueprint for future products.)

Early AIs didn’t know much about the world, and academic departments lacked the computing power to exploit them at scale. The difference today is not intelligence, but data and power. The big tech companies have spent 20 years harvesting vast amounts of data from culture and everyday life, and building vast, energy-hungry data centres filled with ever more powerful computers to churn through it. What were once creaky old neural networks have become super-powered, and the gush of AI we’re seeing is the result.

AI image generation relies on the assembly and analysis of millions upon millions of tagged images; that is, images that come with some kind of description of their content already attached. These images and descriptions are then processed through neural networks that learn to associate particular and deeply nuanced qualities of the image – shapes, colours, compositions – with certain words and phrases. These qualities are then layered on top of one another to produce new arrangements of shape, colour and composition, based on the billions of differently weighted associations produced by a simple prompt. But where did all those original images come from?

The datasets released by LAION, a German non-profit, are a good example of the kind of image-text collections used to train large AI models (they provided the basis for both Stable Diffusion and Google’s Imagen, among others). For more than a decade, another non-profit web organisation, Common Crawl, has been indexing and storing as much of the public world wide web as it can access, filing away as many as 3bn pages every month. Researchers at LAION took a chunk of the Common Crawl data and pulled out every image with an “alt” tag, a line or so of text meant to be used to describe images on web pages. After some trimming, links to the original images and the text describing them are released in vast collections: LAION-5B, released in March 2022, contains more than five billion text-image pairs. These images are “public” images in the broadest sense: any image ever published on the internet may be gathered up into them, with exactly the kind of strange effects one may expect.

In September 2022, a San Francisco–based digital artist named Lapine was using a tool called Have I Been Trained, which allows artists to see if their work is being used to train AI image generation models. Have I Been Trained was created by the artists Mat Dryhurst and Holly Herndon, whose own work led them to explore the ways in which artists’ labour is coopted by AI. When Lapine used it to scan the LAION database, she found an image of her own face. She was able to trace this image back to photographs taken by a doctor when she was undergoing treatment for a rare genetic condition. The photographs were taken as part of her clinical documentation, and she signed documents that restricted their use to her medical file alone. The doctor involved died in 2018. Somehow, these private medical images ended up online, then in Common Crawl’s archive and LAION’s dataset, and were finally ingested into the neural networks as they learned about the meaning of images, and how to make new ones. For all we know, the mottled pink texture of our Saint-Exupéry-style piggy could have been blended, however subtly, from the raw flesh of a cancer patient.

“It’s the digital equivalent of receiving stolen property. Someone stole the image from my deceased doctor’s files and it ended up somewhere online, and then it was scraped into this dataset,” Lapine told the website Ars Technica. “It’s bad enough to have a photo leaked, but now it’s part of a product. And this goes for anyone’s photos, medical record or not. And the future abuse potential is really high.” (According to her Twitter account, Lapine continues to use tools like Dall-E to make her own art.)

The entirety of this kind of publicly available AI, whether it works with images or words, as well as the many data-driven applications like it, is based on this wholesale appropriation of existing culture, the scope of which we can barely comprehend. Public or private, legal or otherwise, most of the text and images scraped up by these systems exist in the nebulous domain of “fair use” (permitted in the US, but questionable if not outright illegal in the EU). Like most of what goes on inside advanced neural networks, it’s really impossible to understand how they work from the outside, rare encounters such as Lapine’s aside. But we can be certain of this: far from being the magical, novel creations of brilliant machines, the outputs of this kind of AI is entirely dependent on the uncredited and unremunerated work of generations of human artists.

AI image and text generation is pure primitive accumulation: expropriation of labour from the many for the enrichment and advancement of a few Silicon Valley technology companies and their billionaire owners. These companies made their money by inserting themselves into every aspect of everyday life, including the most personal and creative areas of our lives: our secret passions, our private conversations, our likenesses and our dreams. They enclosed our imaginations in much the same manner as landlords and robber barons enclosed once-common lands. They promised that in doing so they would open up new realms of human experience, give us access to all human knowledge, and create new kinds of human connection. Instead, they are selling us back our dreams repackaged as the products of machines, with the only promise being that they’ll make even more money advertising on the back of them.






        </section>
        <!-- Repeat the section structure for other parts of your article -->
    </article>
    <div id="symbol-counter"></div>
    <script>
      // JavaScript to count symbols and update the counter
      const articleContent = document.querySelector('article');
      const symbolCounter = document.querySelector('#symbol-counter');

      function updateSymbolCount() {
          const content = articleContent.textContent || articleContent.innerText;
          const symbolCount = content.length;
          symbolCounter.textContent = `Total symbols: ${symbolCount}`;
      }

      updateSymbolCount();
  </script>
</body>
</html>
