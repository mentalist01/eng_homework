<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>What Happens When an AI Knows How You Feel?</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f2f2f2;
        }

        header {
            background-color: #333;
            color: #fff;
            padding: 20px;
            text-align: center;
        }

        h1 {
            font-size: 24px;
            margin: 0;
        }

        article {
            max-width: 800px;
            margin: 20px auto;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
        }

        section {
            margin-bottom: 20px;
        }

        h2 {
            font-size: 20px;
            margin-top: 0;
        }

        p {
            font-size: 16px;
            line-height: 1.5;
        }
        #symbol-counter {
            font-size: 14px;
            color: #888;
            text-align: right;
        }
    </style>
</head>
<body>
    <header>
        <h1>What Happens When an AI Knows How You Feel?</h1>
    </header>
    <article>
        <section>
          IN MAY 2021, Twitter, a platform notorious for abuse and hot-headedness, rolled out a “prompts” feature that suggests users think twice before sending a tweet. The following month, Facebook announced AI “conflict alerts” for groups, so that admins can take action where there may be “contentious or unhealthy conversations taking place.” Email and messaging smart-replies finish billions of sentences for us every day. Amazon’s Halo, launched in 2020, is a fitness band that monitors the tone of your voice. Wellness is no longer just the tracking of a heartbeat or the counting of steps, but the way we come across to those around us. Algorithmic therapeutic tools are being developed to predict and prevent negative behavior.

Jeff Hancock, a professor of communication at Stanford University, defines AI-mediated communication as when “an intelligent agent operates on behalf of a communicator by modifying, augmenting, or generating messages to accomplish communication goals.” This technology, he says, is already deployed at scale.

Beneath it all is a burgeoning belief that our relationships are just a nudge away from perfection. Since the start of the pandemic, more of our relationships depend on computer-mediated channels. Amid a churning ocean of online spats, toxic Slack messages, and infinite Zoom, could algorithms help us be nicer to each other? Can an app read our feelings better than we can? Or does outsourcing our communications to AI chip away at what makes a human relationship human?

Coding Co-Parenting
YOU COULD SAY that Jai Kissoon grew up in the family court system. Or, at least, around it. His mother, Kathleen Kissoon, was a family law attorney, and when he was a teenager he’d hang out at her office in Minneapolis, Minnesota, and help collate documents. This was a time before “fancy copy machines,” and while Kissoon shuffled through the endless stacks of paper that flutter through the corridors of a law firm, he’d overhear stories about the many ways families could fall apart.

In that sense, not much has changed for Kissoon, who is cofounder of OurFamilyWizard, a scheduling and communication tool for divorced and co-parenting couples that launched in 2001. It was Kathleen’s concept, while Jai developed the business plan, initially launching OurFamilyWizard as a website. It soon caught the attention of those working in the legal system, including Judge James Swenson, who ran a pilot program with the platform at the family court in Hennepin County, Minneapolis, in 2003. The project took 40 of what Kissoon says were the “most hardcore families,” set them up on the platform—and “they disappeared from the court system.” When someone eventually did end up in court—two years later—it was after a parent had stopped using it.

Two decades on, OurFamilyWizard has been used by around a million people and gained court approval across the US. In 2015 it launched in the UK and a year later in Australia. It’s now in 75 countries; similar products include coParenter, Cozi, Amicable, and TalkingParents. Brian Karpf, secretary of the American Bar Association, Family Law Section, says that many lawyers now recommend co-parenting apps as standard practice, especially when they want to have a “chilling effect” on how a couple communicates. These apps can be a deterrent for harassment and their use in communications can be court-ordered.
<br> <br>
In a bid to encourage civility, AI has become an increasingly prominent feature. OurFamilyWizard has a “ToneMeter” function that uses sentiment analysis to monitor messages sent on the app— “something to give a yield sign,” says Kissoon. Sentiment analysis is a subset of natural language processing, the analysis of human speech. Trained on vast language databases, these algorithms break down text and score it for sentiment and emotion based on the words and phrases it contains. In the case of the ToneMeter, if an emotionally charged phrase is detected in a message, a set of signal-strength bars will go red and the problem words are flagged. “It’s your fault that we were late,” for example, could be flagged as “aggressive.” Other phrases could be flagged as being “humiliating” or “upsetting.” It’s up to the user if they still want to hit send.

ToneMeter was originally used in the messaging service, but is now being coded for all points of exchange between parents in the app. Shane Helget, chief product officer, says that soon it will not only discourage negative communication, but encourage positive language too. He is gathering insights from a vast array of interactions with a view that the app could be used to proactively nudge parents to behave positively toward each other beyond regular conversations. There could be reminders to communicate schedules in advance, or offer to swap dates for birthdays or holidays—gestures that may not be required but could be well-received.

CoParenter, which launched in 2019, also uses sentiment analysis. Parents negotiate via text and a warning pops up if a message is too hostile—much like a human mediator might shush their client. If the system does not lead to an agreement, there is the option to bring a human into the chat.

Deferring to an app for such emotionally fraught negotiations is not without issues. Kissoon was conscious not to allow the ToneMeter to score parents on how positive or negative they seem, and Karpf says he has seen a definite effect on users’ behavior. “​​The communications become more robotic,” he says. “You’re now writing for an audience, right?”

Co-parenting apps might be able to help steer a problem relationship, but they can’t solve it. Sometimes, they can make it worse. Karpf says some parents weaponize the app and send “bait” messages to wind up their spouse and goad them into sending a problem message: “A jerk parent is always going to be a jerk parent”. Kisson recalls a conversation he had with a judge when he launched the pilot program. “The thing to remember about tools is that I can give you a screwdriver and you can fix a bunch of stuff with it,” the judge said. “Or you can go poke yourself in the eye.”

Computer Says Hug
IN 2017, ADELA TIMMONS was a doctoral student in psychology undertaking a clinical internship at UC San Francisco and San Francisco General Hospital, where she worked with families that had young children from low-income backgrounds who had been exposed to trauma. While there, she noticed a pattern emerging: Patients would make progress in therapy only for it to be lost in the chaos of everyday life between sessions. She believed technology could “bridge the gap between the therapist’s room and the real world” and saw the potential for wearable tech that could intervene just at the moment a problem is unfolding.

In the field, this is a “Just in Time Adaptive Intervention.” In theory, it’s like having a therapist ready to whisper in your ear when an emotional alarm bell rings. “But to do this effectively,” says Timmons, now director of the Technological Interventions for Ecological Systems (TIES) Lab at Florida International University, “you have to sense behaviors of interest, or detect them remotely.”
<br> <br>
Timmons’ research, which involves building computational models of human behavior, is focused on creating algorithms that can effectively predict behavior in couples and families. Initially she focused on couples. For one study, researchers wired up 34 young couples with wrist and chest monitors and tracked body temperature, heartbeat and perspiration. They also gave them smartphones that listened in on their conversations. By cross-referencing this data with hourly surveys in which the couples described their emotional state and any arguments they had, Timmons and her team developed models to determine when a couple had a high chance of fighting. Trigger factors would be a high heart rate, frequent use of words like “you,” and contextual elements, such as the time of day or the amount of light in a room. “There isn’t one single variable that counts as a strong indicator of an inevitable row,” Timmons explains (though driving in LA traffic was one major factor), “but when you have a lot of different pieces of information that are used in a model, in combination, you can get closer to having accuracy levels for an algorithm that would really work in the real world.”
<br><br>
Timmons’ research, which involves building computational models of human behavior, is focused on creating algorithms that can effectively predict behavior in couples and families. Initially she focused on couples. For one study, researchers wired up 34 young couples with wrist and chest monitors and tracked body temperature, heartbeat and perspiration. They also gave them smartphones that listened in on their conversations. By cross-referencing this data with hourly surveys in which the couples described their emotional state and any arguments they had, Timmons and her team developed models to determine when a couple had a high chance of fighting. Trigger factors would be a high heart rate, frequent use of words like “you,” and contextual elements, such as the time of day or the amount of light in a room. “There isn’t one single variable that counts as a strong indicator of an inevitable row,” Timmons explains (though driving in LA traffic was one major factor), “but when you have a lot of different pieces of information that are used in a model, in combination, you can get closer to having accuracy levels for an algorithm that would really work in the real world.”
The next step is to test the models in real time to see if they are effective and whether prompts from a mobile phone actually lead to meaningful behavioral change. “We have a lot of good reasons and theories to think that would be a really powerful mechanism of intervention,” Timmons says. “We just don’t yet know how well they work in the real world.”


        </section>
        <!-- Repeat the section structure for other parts of your article -->
    </article>
    <div id="symbol-counter"></div>
    <script>
      // JavaScript to count symbols and update the counter
      const articleContent = document.querySelector('article');
      const symbolCounter = document.querySelector('#symbol-counter');

      function updateSymbolCount() {
          const content = articleContent.textContent || articleContent.innerText;
          const symbolCount = content.length;
          symbolCounter.textContent = `Total symbols: ${symbolCount}`;
      }

      updateSymbolCount();
  </script>
</body>
</html>
